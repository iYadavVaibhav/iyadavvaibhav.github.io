I"ËÉ<p>Statistics is used to summarize data. Probability is a way to find likeliness of an event to happen. Permutation and combination are ways to count events and possibilities. All these learning are from <a href="https://www.khanacademy.org/math/statistics-probability">Khan Academy</a> and <a href="https://www.isb.edu/advanced-management-programme-in-business-analytics">AMPBA - ISB</a>.</p>

<ul id="markdown-toc">
  <li><a href="#probability" id="markdown-toc-probability">Probability</a>    <ul>
      <li><a href="#events" id="markdown-toc-events">Events</a></li>
      <li><a href="#addition-rule" id="markdown-toc-addition-rule">Addition Rule</a></li>
      <li><a href="#multiplication-rule" id="markdown-toc-multiplication-rule">Multiplication Rule</a></li>
      <li><a href="#conditional-probability" id="markdown-toc-conditional-probability">Conditional Probability</a>        <ul>
          <li><a href="#dependent-events" id="markdown-toc-dependent-events">Dependent Events</a></li>
          <li><a href="#independent-events" id="markdown-toc-independent-events">Independent Events</a></li>
        </ul>
      </li>
      <li><a href="#counting-events" id="markdown-toc-counting-events">Counting Events</a>        <ul>
          <li><a href="#permutation" id="markdown-toc-permutation">Permutation</a></li>
          <li><a href="#combinations" id="markdown-toc-combinations">Combinations</a></li>
        </ul>
      </li>
      <li><a href="#approach-to-solve-a-problem" id="markdown-toc-approach-to-solve-a-problem">Approach to solve a problem</a></li>
    </ul>
  </li>
  <li><a href="#statistics" id="markdown-toc-statistics">Statistics</a>    <ul>
      <li><a href="#summarizing-quantitative-data" id="markdown-toc-summarizing-quantitative-data">Summarizing Quantitative Data</a>        <ul>
          <li><a href="#effect-of-outliers" id="markdown-toc-effect-of-outliers">Effect of Outliers</a></li>
        </ul>
      </li>
      <li><a href="#spread-and-variation-of-data" id="markdown-toc-spread-and-variation-of-data">Spread and Variation of Data</a></li>
      <li><a href="#measure-of-discrete-variables" id="markdown-toc-measure-of-discrete-variables">Measure of Discrete Variables</a>        <ul>
          <li><a href="#expected-value" id="markdown-toc-expected-value">Expected Value</a></li>
          <li><a href="#expected-variance" id="markdown-toc-expected-variance">Expected Variance</a></li>
          <li><a href="#expected-standard-deviation" id="markdown-toc-expected-standard-deviation">Expected Standard Deviation</a></li>
        </ul>
      </li>
      <li><a href="#measures-of-relations" id="markdown-toc-measures-of-relations">Measures of Relations</a>        <ul>
          <li><a href="#coefficient-of-variation" id="markdown-toc-coefficient-of-variation">Coefficient of Variation</a></li>
          <li><a href="#covariance" id="markdown-toc-covariance">Covariance</a></li>
          <li><a href="#coefficient-of-correlation" id="markdown-toc-coefficient-of-correlation">Coefficient of Correlation</a></li>
        </ul>
      </li>
      <li><a href="#shape-of-distribution" id="markdown-toc-shape-of-distribution">Shape of Distribution</a>        <ul>
          <li><a href="#skewness" id="markdown-toc-skewness">Skewness</a></li>
          <li><a href="#kurtosis" id="markdown-toc-kurtosis">Kurtosis</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#distributions" id="markdown-toc-distributions">Distributions</a>    <ul>
      <li><a href="#frequency-distribution" id="markdown-toc-frequency-distribution">Frequency Distribution</a></li>
      <li><a href="#probability-distribution-functions-pdf" id="markdown-toc-probability-distribution-functions-pdf">Probability Distribution Functions (PDF)</a>        <ul>
          <li><a href="#bernoulli-distribution" id="markdown-toc-bernoulli-distribution">Bernoulli Distribution</a></li>
          <li><a href="#binomial-probability-distribution" id="markdown-toc-binomial-probability-distribution">Binomial Probability Distribution</a></li>
          <li><a href="#poisson-probability-distribution" id="markdown-toc-poisson-probability-distribution">Poisson Probability Distribution</a></li>
          <li><a href="#uniform-distribution" id="markdown-toc-uniform-distribution">Uniform Distribution</a></li>
          <li><a href="#normal-distribution" id="markdown-toc-normal-distribution">Normal Distribution</a>            <ul>
              <li><a href="#standardized-normal-or-z-score" id="markdown-toc-standardized-normal-or-z-score">Standardized Normal or Z score</a></li>
              <li><a href="#calculating-probabilities" id="markdown-toc-calculating-probabilities">Calculating Probabilities</a></li>
              <li><a href="#empirical-rules" id="markdown-toc-empirical-rules">Empirical Rules</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#sampling--distributions" id="markdown-toc-sampling--distributions">Sampling  Distributions</a>        <ul>
          <li><a href="#developing-a-sampling-distributions" id="markdown-toc-developing-a-sampling-distributions">Developing a Sampling Distributions</a></li>
          <li><a href="#standard-error-of-the-mean" id="markdown-toc-standard-error-of-the-mean">Standard Error of the Mean</a></li>
          <li><a href="#central-limit-theorem" id="markdown-toc-central-limit-theorem">Central Limit Theorem</a></li>
          <li><a href="#point-and-interval-estimates" id="markdown-toc-point-and-interval-estimates">Point and Interval Estimates</a></li>
          <li><a href="#confidence-intervals-and-levels" id="markdown-toc-confidence-intervals-and-levels">Confidence Intervals and Levels</a></li>
          <li><a href="#students-t-distribution" id="markdown-toc-students-t-distribution">Student‚Äôs t Distribution</a></li>
          <li><a href="#sampling-error" id="markdown-toc-sampling-error">Sampling Error</a></li>
          <li><a href="#p-value-needs-elaboration" id="markdown-toc-p-value-needs-elaboration">P Value (needs elaboration)</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="probability">Probability</h1>

<p>Probability is how likely event is going to happen. It is possibility of event that is fundamentally random. Any experiment can have outcome for an event. The possibility of outcome is the probability.</p>

<p>Experiment -&gt; many events -&gt; outcomes</p>

<p>Events make sample space, that is, all possible outcomes.</p>

<p>For example, if we flip a coin we can get either heads or tails. The possibility of heads is 50% and possibility ability of tails is 50%. So the probability of Heads is .5 and probability of tails is also .5, if it is a biased coin.</p>

<p>The analysis of event governed by probability is called statistics.</p>

\[P(e) = \frac{ Possibilities }{ Outcomes }\]

<p><strong>Theoretical  probability</strong> is what can be stated and seems fixed. For example flipping a coin.</p>

<p><strong>Experimental probability</strong> Finding an outcome based on past data and experience example prediction of the score. Probability gives a reasonable predictions about an outcome. It is highly likely but not hundred percent true. Also known as <strong>subjective probability</strong>.</p>

<p><strong>Simulation and Randomness</strong> We can use list of random numbers to simulate our experiment multiple times and average out to find confidence.</p>

<h2 id="events">Events</h2>

<p>Every possible outcome of a variable is an event.</p>

<p><strong>Simple event</strong> is an event described by a single characteristic. For eg, a day in January from all days in 2018.</p>

<p><strong>Joint event</strong> is an event described by two or more characteristics. For eg, a day in January that is also a Wednesday from all days in 2018.</p>

<p><strong>Complement of an event</strong> A (denoted A‚Äô). All events that are not part of event A. For eg, all days from 2018 that are not in January.</p>

<p><strong>Mutually Exclusive</strong> events cannot occur simultaneously. They have no intersection outcomes. Also called <strong>Disjoint Sets</strong>. For eg, A = day in Jan, B = day in Feb. A and B cannot occur simultaneously.</p>

<p>In this, P(A1 U A2 U A3‚Ä¶) = P(A1) + P(A2) + P(A3)‚Ä¶</p>

<p>Also, P(A &amp; B) = 0.</p>

<p><strong>Collectively Exhaustive Events</strong> are those in which:</p>
<ul>
  <li>One of the event must occur</li>
  <li>The set of events covers the entire sample space</li>
  <li>For eg, A = Weekday; B = Weekend; C = January; D = Spring; Events A, B, C and D are collectively exhaustive (but not mutually exclusive ‚Äì a weekday can be in January or in Spring).</li>
</ul>

<p>Events A and B are collectively exhaustive and also mutually exclusive.</p>

<p><strong>Independent Events</strong> are those which are not dependent on each other. That is, occurrence of one does not affect occurrence of another event.</p>

<p><strong>Note:</strong> All mutually exclusive events are dependent but not all dependent events are mutually exclusive.</p>

<h2 id="addition-rule">Addition Rule</h2>
<p>Addition rule of probability.</p>

\[P(A \cup B ) = P(A) + P(B) -P(A \cap B)\]

<p>if mutually exclusive, then $P(A \cap B) = 0$.</p>

<p>And is intersection, or is union.</p>

<p>For eg, P(Jan or Wed) = P(Jan) + P(Wed) - P(Jan and Wed) = 31/365 + 52/365 - 5/365 = 78/365</p>

<h2 id="multiplication-rule">Multiplication Rule</h2>

<p>For independent event, what happened in past event will have no effect on current event. For eg, P(HH) or P(at least 1H in 10 flips).</p>

\[P(HH) = 0.5 \times 0.5\]

<p>P(at least 1H in 10 flips) = 1 - P(All T in 10 flips)</p>

\[1 - (0.5)^{10} = 1023 \div 1024 = 99.9%\]

<h2 id="conditional-probability">Conditional Probability</h2>

<p>When we have to find a probability under a <strong>give</strong>n condition.</p>

<h3 id="dependent-events">Dependent Events</h3>

<p>If dependent, the probability of A and B is:</p>

\[P(A\cap B) = P(A \&amp; B) = P(A) \times P(B|A) = P(B) \times P(A|B)\]

<p>where A|B is ‚ÄòA happening after B‚Äô or ‚Äòconditional prob of A given that B has occurred‚Äô.</p>

<p>Here, B becomes the new sample space, because it‚Äôs A <strong>given B</strong>. Hence,</p>

\[P(A|B) = \frac{P(A\cap B)}{P(B)}\]

<h3 id="independent-events">Independent Events</h3>

<p>if independent (does not affect each other), then</p>

\[P(A \&amp; B) = P(A) \times P(B)\]

\[P(A|B) = P(A)\]

\[P(A \space or \space B) = P(A) + P(B) - P (A \&amp; B)\]

<p>because P(B|A) = P(B), occurrence of A has no effect on B.</p>

<h2 id="counting-events">Counting Events</h2>

<h3 id="permutation">Permutation</h3>

<p>Arrange $n$ people in $k$ seats. To count number of ways in which this can be done we use permutation.</p>

<p>For eg, arrange 6 people in 3 seats, 6.5.4 = 6! / 3! = 120.</p>

\[_nP_k = \frac{n!}{(n - k)!} = n(n-1)... (k  times)\]

<p>Used when order matters and pick once (without replacement).</p>

<p>For eg,</p>

\[_{10}P_3 = 10.9.8\]

<h3 id="combinations">Combinations</h3>

\[_nC_k = \binom{n}{k} = \frac{_nP_k}{k!} = \frac{n!}{k!(n - k)!} = \frac{n(n-1)...[k \space times]}{k!}\]

<p>We divide it by the number of ways in which k people can be arranged in k places, i.e, k! because ABCD and BCDA are same and we are counting this extra.</p>

<p>Order doesn‚Äôt matter, 123 = 312.</p>

<p>For eg,</p>

\[_{10}C_3 = \frac{10.9.8}{3.2.1}\]

<h2 id="approach-to-solve-a-problem">Approach to solve a problem</h2>

<p>We can take following approaches to solve a probability problem</p>

<ol>
  <li>
    <p>use simple definition, 
\(P(e) = \frac{events \space possible}{sample \space space}\)</p>
  </li>
  <li>Make a <strong>Contingency Table</strong> with possibilities.
    <ol>
      <li>To find P(A or B), use P(A)+ P(B) - P(A and B)</li>
      <li>To find P(A and B), simply use (joint event)/total.</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>To find P(A</td>
              <td>B), P(A and B) / P(B)</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ol>
  </li>
  <li>Make a <strong>Decision Tree</strong>, use when question has ‚Äúafter‚Äù.
    <ol>
      <li>Find branches and outcomes</li>
      <li>Find effective value by multiplying with probabilities</li>
      <li>Roll back to find effective value at each branch.</li>
    </ol>
  </li>
  <li>
    <p>Use <strong>Venn Diagram</strong> when and/or is combined with not of a event.</p>
  </li>
  <li>At least or at most, use 
\(P(at least/most) = 1 - P(e)\)</li>
</ol>

<p><strong>Example</strong></p>

<p>Find number of ways to arrange 1 - 10 digits in 3 places,</p>

<p>Repetition allowed, order matters = 10.10.10</p>

<p>Repetition not allowed, order matters = Permutation = 10.9.8</p>

<p>Repetition allowed, order doesn‚Äôt matter =</p>

\[\frac{10.10.10}{3.2.1}\]

<p>Repetition not allowed, order doesn‚Äôt matter = Combination =</p>

\[\frac{10.9.8}{3.2.1}\]

<h1 id="statistics">Statistics</h1>
<p>Basics of statistics for Data Science.</p>

<p><strong>Central tendency</strong> measures tell us how values are grouped around the centre value.</p>

<p><strong>Variation</strong> tells us how disperse the data is or how much it is scattered</p>

<p><strong>Shape</strong> tells us the pattern of distribution of values. How the data is distributed in a frequency.</p>

<p>Two type:</p>
<ul>
  <li>Descriptive</li>
  <li>Inferential</li>
</ul>

<h2 id="summarizing-quantitative-data">Summarizing Quantitative Data</h2>
<p>Mean median and mode are three ways to summarize the data and to measure the central tendency of data. These are usually calculated for quantitative data.</p>

<p><strong>Mean</strong></p>

<p>Average or mean are one and the same thing. It is sum of all the data points divided by number of data points.</p>

\[\mu = \frac{ \sum_{i=0}^n x_i }{ n }\]

<p>It gets affected by the outliers .</p>

<p><strong>Median</strong></p>

<p>It is the middle number in an ordered list.</p>

<p>If it‚Äôs even then it is sum of two middle number divided by 2, else it‚Äôs the middle one.</p>

<p>if $n = even$ then: $(a+b)/2$ where a and b are middle numbers.</p>

<p>It is not affected by the extreme values.</p>

<p><strong>Mode</strong></p>

<p>It is the most <em>common number</em> or the most frequent number in the dataset.</p>

<p><em>No mode</em>, if the values in a given set all occur the same number of times, the data set has no mode because no number is any more common than any other.</p>

<p><em>Several mode</em> if more than one number is repeated same number of times.</p>

<p>It is not affected by extreme values.</p>

<p><strong>Geometric Mean</strong></p>

<p>It is used to measure rate of change of variable over time. For eg, rate of return on investment over years.</p>

\[X_G = (X_1 \ x \  X_2 \ x\ ... \ x\  X_n)^{(1/n)}\]

<p>GM rate of return, here $R_i$ is rate of return in time period i</p>

\[R_G = [ (1 + R_1) \ x \ (1 + R_2) \ x ... x \ (1 + R_n) ]^{(1/n)} - 1\]

<h3 id="effect-of-outliers">Effect of Outliers</h3>

<p>Removing a big outlier decreases the mean more but less change on median and may be median doesn‚Äôt change.</p>

<h2 id="spread-and-variation-of-data">Spread and Variation of Data</h2>
<p>The spread of data can be measured by its range, interquartile range (IQR), variance, standard deviation. and coefficient of Variation.</p>

<p><strong>Range</strong></p>

<p>It is difference between largest and smallest value in data. It is not dependent on distribution of data and is sensitive to outliers.</p>

<p><strong>Interquartile Range</strong></p>

<p>Quartiles divide the ordered data in to 4 segments. Each have equal number of values.  Median (Q2) divides the dataset into two different parts. IQR is the difference in the middle of the first half (Q1) and the middle of second half (Q3). We then find the median of these two different parts and then find the difference.</p>

<p>If we have even number of data points in dataset then we include the first middle number in first set and the second middle number in second set.</p>

<p>IQR is useful to find out how much the data is varying.</p>

<p>Range can be misleading when we have outliers. But in this case interquartile range can give us much better measure of spread of data.</p>

<p>$IQR$ = md 2nd quartile - md 1st quartile = $Q_3 - Q_2$</p>

<p><strong>Quartile Measures</strong></p>

<ul>
  <li>Q1, is the value for which 25% of the observations are smaller and 75% are larger</li>
  <li>Q2 is the same as the median (50% of the observations are smaller and 50% are larger)</li>
  <li>Only 25% of the observations are greater than the Q3</li>
</ul>

<p>A <em>box plot</em> shows Min, Q1, Q2, Q3 and Max values of data.</p>

<p><strong>Measure of spread</strong></p>

<p>Measure of spread can be found by calculating range variance and standard deviation.</p>
<ul>
  <li>Sample is part of data while population is entire dataset.</li>
  <li>We can estimate population by using these measures from the samples.</li>
</ul>

<p><strong>Variance</strong></p>

<p>Squared deviation of values from the mean</p>

\[\sigma^2 = \frac{ \sum(x_i - \mu)^2 }{ n } = \frac{ \sum(x_i)^2 }{ n } - \mu^2\]

<p><strong>Standard Deviation</strong></p>

<p>Variation about the mean. It has <em>same unit as the original data</em>.</p>

\[\sigma = \sqrt{Variance} = \sqrt{\sigma^2} = \sqrt \frac{ \sum_{1}^n(x_i - \mu)^2 }{ n }\]

<p><strong>For example:</strong></p>

<p>A = [-10, 0, 10, 20, 30]</p>

<p>B = [8, 9, 10, 11, 12]</p>

<p>Mean of two data set is same (10) but range varies. So we calculate variance to show difference between datasets.</p>

\[\sigma^2(A) = 200\]

\[\sigma^2(B) = 2\]

\[\sigma(A) = \sqrt{200} = 10\sqrt{2}\]

\[\sigma(B) = \sqrt{2}\]

<p><strong>Result</strong></p>

<p>Hence, points in A are 10 times more deviated.</p>

<p><strong>Note:</strong></p>

<ul>
  <li>The more the spread, the greater is range, variance and SD</li>
  <li>The more data is contracted, the smaller these measures are.</li>
  <li>Standard Deviations tells us how far we are from the mean.</li>
  <li>If all values are same (no variation) then all these are zero.</li>
  <li>None of these measures can ever be negative.</li>
  <li>Adding a number to all values, $x_i$, makes no difference to variance.</li>
  <li>Multiplying a number, k, to all values, $x_i$, makes variance  $\sigma^2 \times k$</li>
</ul>

<p><strong>Better measures</strong></p>

<p>What defines dataset better, mean or median?</p>
<ul>
  <li>Mean can give us the standard deviation, median can you give us IQR.</li>
  <li>Mean is better for symmetric dataset, median is better for skewed dataset which has outliers.</li>
  <li>Median is better for salaries and home prices as it has outliers.</li>
</ul>

<h2 id="measure-of-discrete-variables">Measure of Discrete Variables</h2>

<p>Discrete Variable has a finite outcome. It has a fixed values. The events are <strong>mutually exclusive</strong>.</p>

<h3 id="expected-value">Expected Value</h3>

<p>This is measure of center. Also called <strong>weighted average</strong>.</p>

\[\mu = E(X) = \sum_{i=1}^N X_i P(X=X_i)\]

<p>This is sum of each (value x probability)</p>

<h3 id="expected-variance">Expected Variance</h3>

<p>Difference from mean, squared times probability, then sum:</p>

\[\sigma^2 = \sum_{i=1}^N [X_i - E(X)]^2 P(X=X_i)\]

<h3 id="expected-standard-deviation">Expected Standard Deviation</h3>

<p>Difference from mean, squared times probability, then sum:</p>

\[\sigma = \sqrt{\sigma^2} = \sqrt{ \sum_{i=1}^N [X_i - E(X)]^2 P(X=X_i) }\]

<h2 id="measures-of-relations">Measures of Relations</h2>

<h3 id="coefficient-of-variation">Coefficient of Variation</h3>

<p>Coefficient is the multiplicative factor. That is how many times a variable is of another variable.</p>

<p>Here we compare variation and mean. So it <strong>variation relative to mean</strong>. Always in percentage, and can compare data with different units.</p>

\[CV = \frac{ S }{ X } \times 100\%\]

<p>For example,</p>

<p>Stock A = $50, SD = $5</p>

<p>CV(A) = (5/50)*100% = 10%</p>

<p>Stock B = $100, SD = $5</p>

<p>CV(B) = (5/100)*100% = 5%</p>

<p>Hence, both stocks have same SD, but stock B is less variable relative to its price.</p>

<h3 id="covariance">Covariance</h3>

<p>Measure of strength of linear relationship between two discrete random variables X and Y.</p>

\[\sigma_{XY} = \sum_{i=1}^N [X_i - E(X)][Y_i - E(Y)] P(X=X_i,Y=Y_i)\]

<p>where, $P(X=X_i,Y=Y_i)$ is probability of occurrence of the i outcome of X and the i outcome of Y.</p>

<h3 id="coefficient-of-correlation">Coefficient of Correlation</h3>

<p>The relative strength of linear relation between two variables is called correlation. It is between -1 and 1.</p>

\[r = \frac{cov(X,Y)}{S_XS_Y}\]

<p>This is covariance relative to standard deviation. In above,</p>

<p>\(cov(X,Y) =  \frac{\sum_{i=1}^N [X_i - \overline{X}][Y_i - \overline{Y}]}{n-1}\)
\(S_X =  \sqrt{ \frac{\sum_{i=1}^N [X_i - \overline{X}]^2}{n-1} }\)
\(S_Y =  \sqrt{ \frac{\sum_{i=1}^N [Y_i - \overline{Y}]^2}{n-1} }\)</p>

<p>Cor = 0, means no linear relation, but may has non-linear relation.</p>

<p>In investment portfolio, expected return and standard deviation of two funds together can be calculated.</p>

<h2 id="shape-of-distribution">Shape of Distribution</h2>

<p>This tells us how data is distributed. It can be measure by:</p>
<ul>
  <li>Skewness: Extent to which data values are not symmetrical</li>
  <li>Kurtosis: Affects the peakedness of the curve of the distribution. It tell the sharpness of rise of curve. Bell shaped is Mesokurtic (Kurtosis = 0).</li>
</ul>

<h3 id="skewness">Skewness</h3>

<p>Data can be symmetric, left or right skewed. If it is symmetric then we can work on half of the sample of data.</p>

<p>Left: Mean &lt; Median
Symmetric: Mean = Median
Right: Median &lt; Mean</p>

<h3 id="kurtosis">Kurtosis</h3>

<p>How sharply the curve rises. Eg:</p>
<ul>
  <li>High rise, Kurtosis &gt; 0, Leptokurtic.</li>
  <li>Bell shaped, Kurtosis = 0, Mesokurtic.</li>
  <li>Low rise, Kurtosis &lt; 0, Platykurtic.</li>
</ul>

<blockquote>
  <p>Image of shape and quartiles and box plot</p>
</blockquote>

<h1 id="distributions">Distributions</h1>

<h2 id="frequency-distribution">Frequency Distribution</h2>

<p>We can break data into different classes, then find frequency of data in each class. It is what we see in histogram.</p>

<p>Each class has a mid point, a frequency. We can find <strong>Relative Frequency</strong> which is frequency divided by total frequency (number of data points). So this give us percentage of data point is a particular class interval.</p>

<p>For eg, class A (10 but less than 20) has frequency 3, total observations 20, so relative freq 0.15, hence, 15% data points are in class A.</p>

<p>We can also find cumulative frequency and relative cumulative frequency or <strong>Cumulative Percentage</strong>. It can help us find probability that a data is under or less than that class interval.</p>

<p><strong>Use of Frequency Distribution</strong></p>

<ul>
  <li>Raw to useful form</li>
  <li>Visually see the data distribution</li>
  <li>See interval in which data is contracted or clustered.</li>
</ul>

<p><strong>Tips:</strong></p>

<ul>
  <li>Play with class interval to see different picture of data.</li>
  <li>In large dataset, boundaries don‚Äôt make much difference.</li>
  <li>When comparing different groups with sample, use relative frequency or percentage distribution.</li>
</ul>

<h2 id="probability-distribution-functions-pdf">Probability Distribution Functions (PDF)</h2>

<p>A frequency distribution can be a probability distribution if the area under the curve is equal to 1. The f(x) of Prob Dist gives us probability of occurrence of x in the given distribution.</p>

<p><strong>Random Variables</strong> RV, $X$, are variables that can have any random value $x$. For eg. rolling a die.</p>

<p>$P(X=x)$ is probability that X has outcome x.</p>

<p>$F(x)$ = PDF</p>

<p>$E(x)$ is expected value = weighted average</p>

<p>RV can be of two types:</p>
<ul>
  <li>Discreet, fixed values, finite outcome, mutually exclusive events.</li>
  <li>Continuous, any value in range, data points are approx values.</li>
</ul>

<p>Discreet variables PDF:</p>
<ul>
  <li>Bernoulli Distribution</li>
  <li>Binomial Distribution</li>
  <li>Poisson Distribution</li>
</ul>

<h3 id="bernoulli-distribution">Bernoulli Distribution</h3>

<p>The event has two outcomes, so probabilities are $p$ and $p-1$.</p>

<h3 id="binomial-probability-distribution">Binomial Probability Distribution</h3>

<p>When we have to make $k$ success in $n$ attempts, then we can following formula:</p>

\[P = _nC_k . œÄ^k . (1-œÄ)^{(n-k)}\]

<p>Here œÄ is probability to get success. eg, throwing a basket ball.</p>

<p>Now if we move k from 0 to n, we get different values of P. This can make a distribution. We call this binomial distribution.</p>

<h3 id="poisson-probability-distribution">Poisson Probability Distribution</h3>

<p>Continuous RV can have PDF:</p>
<ul>
  <li>Uniform or</li>
  <li>Normal.</li>
</ul>

<h3 id="uniform-distribution">Uniform Distribution</h3>

<p>Here, probability of each outcome is equal. Hence, funcition is 1/range:</p>

<p>\(f(X) = \frac{1}{(b - a)}\) 
where a is min and b is max.</p>

<p>Measures:</p>

\[\mu = \frac{a + b}{2}\]

\[\sigma = \sqrt{  \frac{(b - a)^2}{ 12 }  }\]

<p>Probability that 3 &lt;= x &lt;= 5, is area under the line between 3 and 5. As it is a rectangle, the area can be (base)(height).</p>

<h3 id="normal-distribution">Normal Distribution</h3>

<p>It is:</p>
<ul>
  <li>Bell Shaped</li>
  <li>Symmetrical</li>
  <li>Mean, Median and Mode are equal</li>
</ul>

<p>The random variable has theoretically infinite range.</p>

<p>It is defined by:</p>

\[f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{X - \mu}{\sigma})^2}\]

<p>e = 2.71828
$\pi$ = 3.14159</p>

<p>Mean moves distribution left/right, sd increases/decreases spread.</p>

<p>P(-‚àû &lt; X &lt; Œº) = 0.5 and P(Œº &lt; X &lt; ‚àû) = 0.5</p>

<h4 id="standardized-normal-or-z-score">Standardized Normal or Z score</h4>

<p>Any normal distribution can be transformed to <strong>standard normal distribution</strong> $(Z)$ using mean and sd.</p>

<p>Need to transform X units to Z units.</p>

<p>It has mean of 0 and SD of 1.</p>

\[z = \frac{X - \mu}{\sigma}\]

<p>Also, known as <em>Z distribution</em>.</p>

<p>For eg, if mean=100, sd=50 then Z for X=$200, (200-100)/50 = 2</p>

<p>This says that X = 200 is two standard deviations (2 increments of 50 units) above the mean of 100.</p>

<p>So for X and Z distribution the shape remains the same, only scale changes.</p>

<p><strong>Note:</strong> We measure a few scores compared to the mean. Like performance in class. How far are we from the mean in terms of SD. are we 1sd far from mean or so on. One such score is Z score.</p>

<h4 id="calculating-probabilities">Calculating Probabilities</h4>

<p>The probability is area under the curve. So P(a &lt;= X &lt;= b) is area under curve between x=a and x=b.</p>

<p><strong>Note:</strong></p>
<ul>
  <li>P(a &lt;= X &lt;= b) = P(a &lt; X &lt; b), as P(a) or P(any point) = 0.</li>
  <li>The total area under the probability curve is 1 and curve is symmetric so half above mean and half below.</li>
</ul>

<p>To calculate probability of any RV in a range we need to find area under the curve in that range. So easily find the answers we convert every normal distribution to standard Z distribution. Then from Z Table we look up values for that value of Z. The score at a value gives area from -$\infty$ till that value.</p>

<p><strong>Z Table</strong> is cumulative probability of standardized normal.</p>

<p>For eg, Z(2) = 0.9772, this is P(Z &lt; 2, till -infinity)</p>

<p>To find a probability for normal X distribution, convert X to Z the find Z value for it.</p>

<h4 id="empirical-rules">Empirical Rules</h4>

<p>1œÉ covers 68.26% of X</p>

<p>2œÉ covers 95.44% of X</p>

<p>3œÉ covers 99.7% of X</p>

<h2 id="sampling--distributions">Sampling  Distributions</h2>

<p>Population is the large group of data that we want to study. We pick a sample of people and try to compare how they perform compared to the population. We collect different sample from the population for example 50 different records from a population.</p>

<p>There will be variation in each of these different samples. So each of these sample gives us slight variation from each other. So sample A is different from sample B and so on. When we collect different samples and find their mean or sd, then this set of information makes sample distribution.</p>

<h3 id="developing-a-sampling-distributions">Developing a Sampling Distributions</h3>

<p>If I choose <strong>every possible</strong> sample of size ‚Äún‚Äù from a population then I get ‚Äúsampling distribution‚Äù. Now if we collect mean of each of these possible sample then we get ‚ÄúSampling distribution of Sample Means‚Äù.</p>

<p>When we make probability distribution of a population that is unbiased, there are equally likely chance to pick a number. So it is a uniform distribution.</p>

<p>Next, we take all possible sample and find mean of each. The prob distribution of the mean of sample will be a normal distribution.</p>

<p>So we find $\mu_{\overline{X}}$ and $\sigma_{\overline{X}}$ and n, here n is number of items in each sample.</p>

<h3 id="standard-error-of-the-mean">Standard Error of the Mean</h3>

<p>Standard error is standard deviation of mean of means. For eg, if we take random samples from a population, we get mean for each sample. S(A) give mean X(A) and so on. Now all these means (X(A), X(B), X(C)..) for a distribution. The standard deviation of this distribution gives us Standard Error.</p>

<p>It is variability in the mean from sample to sample of same size.</p>

<p>It is standard deviation of means of samples.</p>

\[\sigma_{\overline{X}} = \frac{\sigma}{\sqrt{n}}\]

<p><strong>Note:</strong> The standard error of the mean decreases as the sample size increases.</p>

<h3 id="central-limit-theorem">Central Limit Theorem</h3>

<p>It says that the distribution of mean of samples is mostly normal distribution. It is not dependent on shape of original population.</p>

<h3 id="point-and-interval-estimates">Point and Interval Estimates</h3>

<p>Point is a single number, X, in population. We can find confidence interval for that number. Interval estimate tells us more information about the population than a point estimate tells us.</p>

<p>So for eg, 5kg rice packet has SD of 50gm in population. So 3SD would contain 99% of rice packets. Now if we take samples and find mean of samples it would give us a distribution. We can take one Point near 5kg, then we can find the confidence interval in which this mean will lie with a percentage confidence. Like, 95% confidence that x=49.94 is mean and lies between found interval.</p>

<p><strong>Point estimate $\pm$ (Critical Value)(Standard Error)</strong></p>

<p>Point is the sample statistic estimating population parameter of interest.</p>

<p>Critical Value is z table value, based on confidence interval</p>

<p>Standard error is SD of point estimate.</p>

<p>100% is close to infinity, 100% sure that mean will lie between infinities. 95% is also large interval. 0% confidence would be a very small interval.</p>

<h3 id="confidence-intervals-and-levels">Confidence Intervals and Levels</h3>

<p>95% confidence interval = (1 - Œ±) = 0.95</p>

<p>=&gt; Œ±    = 0.05</p>

<p>=&gt; Œ±/2  = 0.025</p>

<p>We are interested in points where the probability left out is 0.025. So from point unto -infinity the probability is 0.025.</p>

<p>Hence, in Z table, the score 0.025 can be found for Z value 1.96.</p>

<p>Here, $Z_{\alpha/2}=\pm1.96$, this is the normal distribution critical value for a probability of Œ±/2 in each tail.</p>

<p>This tells us that on a standard curve, sd=1 and mean=0, the 95% confidence interval is +- 1.96. To find actual values of actual normal curve, we use the formula Point estimate $\pm$ (Critical Value)(Standard Error).</p>

<p>90% = 1.645</p>

<p>95% = 1.96</p>

<p>99% = 2.58</p>

<p>This also tells us that 95% of intervals contains Œº.</p>

<p><strong>CI, when œÉ is known:</strong></p>

\[\overline{X} \pm Z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\]

<p><strong>CI when œÉ is unknown</strong></p>

<p>We can substitute the sample standard deviation, S.This introduces extra uncertainty, since S is variable from sample to sample. So we use the t distribution instead of the normal distribution.</p>

<h3 id="students-t-distribution">Student‚Äôs t Distribution</h3>

<p>It is a family of distributions. Its value depends on degrees of freedom (d.f) = n - 1.</p>

<p>Degree of freedom is a number of observation that can take any value after we have mean calculated.</p>

<p>t -&gt; Z as n increases.</p>

\[\overline{X} \pm t_{\alpha/2} \frac{S}{\sqrt{n}}\]

<p>where $t_{Œ±/2}$ is the critical value of the t distribution with $n-1$ degrees
of freedom and an area of Œ±/2 in each tail</p>

<h3 id="sampling-error">Sampling Error</h3>

<p>There is an error associated with mean of a sample. We can find a sample size to get desired <em>margin of error (e)</em> with (1 - Œ±) level of confidence.</p>

\[e = Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\]

\[n = \frac{Z_{\alpha/2}^2 \sigma^2}{e^2}\]

<p>For eg, what can be the sample size if we want error of $\pm$ 5 with 95% confidence interval when œÉ=20.</p>

<h3 id="p-value-needs-elaboration">P Value (needs elaboration)</h3>

<p>It is same as probability but also take equally likely outcome and rare outcome. So p(two heads, HH) = 0.25. p value of HH is P(HH) + P(TT) + 0 for rare.</p>

:ET