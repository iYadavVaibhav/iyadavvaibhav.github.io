---
layout: post
title: Data Science Learning Notes 
categories: notes datascience
last_modified_at: 2021-09-25 08:41:44
---

Table of Index:

Maths:
- [Probability](https://www.kaggle.com/iyadavvaibhav/probability-notes)
- [Statistics](https://www.kaggle.com/iyadavvaibhav/statistics-the-basics)
- [Statistics Advance](https://www.kaggle.com/iyadavvaibhav/statistics-distributions-nrml-clt-conf-int) - Distributions

Data ETL EDA Wrangling
- [Python Notes](https://www.kaggle.com/iyadavvaibhav/python-notes) - Data Structures
- [Pandas Notes](https://www.kaggle.com/iyadavvaibhav/pandas-notes) - Series, DataFrame, Heirarchy
- [Data Handling](https://www.kaggle.com/iyadavvaibhav/data-handling-notes/) - Pre-Processing, EDA, Transformation
- Dimentionality Reduction - PCA, LDA, Kernel PCA

[Regression](https://www.kaggle.com/iyadavvaibhav/ml-regression-theory) - supervised:
  - Linear Regression
  - Simple Linear Regression
  - Multiple Linear Regression
  - Polynomial Linear Regression
  - Support Vector Regression
  - Decision Tree Regression
  - Random Forest Regression

[Classification](https://www.kaggle.com/iyadavvaibhav/ml-classification-theory) - supervised:
- Logistic Regression - confusion matrix, accuracy, sigmoid, CAP Curve
- KNN - K Nearest Neighbour Classifier - Euclidean distance
- SVM - Support Vector Machines - Maximum Margin Hyperplane
- Kernel SVM - Map to Higher Dimention, Kernel Trick, Gaussian RBF Kernel
- Naive Bayes - Bayes Theorem
- Decision Tree Classifier
- Random Forest Classifier - entropy, ensemble learning

[Clustering](https://www.kaggle.com/iyadavvaibhav/ml-clustering-theory) - unsupervised:
- K-Means Clustering - wcss, choosing k-value, k-means++
- Hierarchical Clustering - Agglomerative, Dendrogram

[Association Rule](https://www.kaggle.com/iyadavvaibhav/ml-association-rule-learning-notes) - Unsupervised:
- Apriori Algorithm - market basket analysis, lift, support, confidence, todo
- Eclat (Part 5 todo)
- FP Growth

Other Bonus Extra:
- Model Select 
- Reinforcement Learning (Part 6 todo) 
  - Upper Confidence Bound
  - Thompson Sampling

[NLP](https://www.kaggle.com/iyadavvaibhav/ml-nlp-notes):
- Cleaning, stemming, nltk, bag of words, tokenization

Deep Learning: (Part 8 todo)

[Dimentionality Reduction](https://www.kaggle.com/iyadavvaibhav/ml-dimentionality-reduction-notes):
- PCA - max variance, unsupervised
- LDA - max class separation, supervised
- Kernel PCA - kernel trick on non-linearly separable dataset
- SVD
- GDA - Generalized Discriminant Analysis